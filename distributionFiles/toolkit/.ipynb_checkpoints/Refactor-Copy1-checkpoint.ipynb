{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usehttp://localhost:8888/notebooks/work/toolkit/caseStudy.ipynb#First,-we-gather-the-dataset.-This-is-a-history-of-static-code-metrics-(C-and-indentation)-and-change-metrics-for-our-project.d to access the toolkit modules in this directory\n",
    "import toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we gather the dataset. This is a history of static code metrics (Java, C and indentation) and change metrics for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/jacobgarcia/linked-lists.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to indicate where the data should be gathered and stored\n",
    "\n",
    "rootDirectory = '../dataSets/linkedListsStudy/'\n",
    "metricsDataList = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/jacobgarcia/linked-lists.git', rootDirectory+'linked-lists/', '*/*.java *.java', ['java'], skipEvery=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricsDataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to indicate where the data should be gathered and stored\n",
    "rootDirectory = '../dataSets/springFrameworkStudy/'\n",
    "\n",
    "# Call gatherTimeMetrics and measure Java, Indent and Change metrics \n",
    "# on .java files from the git project's repository\n",
    "#metricsData = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/spring-projects/spring-framework.git', rootDirectory+'spring-framework/', '*/*.java *.java', ['indent','java'], skipEvery=1000)\n",
    "\n",
    "\n",
    "rootDirectory = '../dataSets/springFrameworkStudy/'\n",
    "metricsDataSpring = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/spring-projects/spring-framework.git', rootDirectory+'spring-framework/', '*/*.java *.java', ['indent','java'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataOpenSSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many times did we sample from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metricsData['times']\n",
    "print metricsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many features and samples are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metricsData['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique source files were measured?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metricsData['data']['entity'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metricsData['data'].head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what affects the net churn of files \n",
    "### Which types of files have net churn above and below the mean net churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate the scikit-learn decision tree classification model\n",
    "# It is trained with a maximum number of leaf nodes\n",
    "# Samples are binned so as to maximize information gain at higher nodes ('entropy')\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "modelInstance = DecisionTreeClassifier(max_leaf_nodes=8, criterion='entropy')\n",
    "modelSimpler = DecisionTreeClassifier(max_leaf_nodes=4, criterion='entropy')\n",
    "churnModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, metricsData['data'], 2, 'netchurn', modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some observations:\n",
    "- The model has very good Precision, Recall and F1-Score: net churn above/below the mean is classified very well by this model\n",
    "- ROC area under curve is very high: very little compromise between false negative rate and false positive rate\n",
    "- The model says that the features influencing net churn are (strongest to weakest):\n",
    "    - Number of lines added\n",
    "    - Number of lines deleted    \n",
    "- The 'net churn below the mean' class is over-represented in our data (3 times as many samples as the other class)\n",
    "    - However, the model still performs well without any steps taken to address class imbalance (e.g. under/over-sampling)\n",
    "- Interpretation of the visualized decision tree is straightforward:\n",
    "    - 63% of samples were files with less than 34 lines added \n",
    "        - These samples had net churn less than the mean\n",
    "        - Some of these may be very stable files (over the history of the project)\n",
    "    - The files with net churn greater than the mean had more than 108 lines added\n",
    "    - Within this group, there are several subgroups with varying levels of churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the above step, we only performed 2-fold cross validation (1 training set, 1 test set)\n",
    "\n",
    "### How does this approach perform with more cross-validation folds in time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into 5 equally-sized groups, \n",
    "# then perform cross-validation while gradually adding these groups to the training set\n",
    "\n",
    "# i.e. the train-test splits are with groups of size:\n",
    "# 1-4, 2-3, 3-2, 4-1\n",
    "\n",
    "# We omit visualization of decision trees to save space,\n",
    "# but they can be shown with visualize=True as above\n",
    "folds = 5\n",
    "churnModelMoreFolds = toolkit.refinement.makeAndUpdateModel(rootDirectory, metricsData['data'], folds, 'netchurn', modelInstance, modelSimpler, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each of the subsets still exhibits class imbalance (but not with the same ratio)\n",
    "### In particular, the 2nd train-test split has the most balanced classes (2:1) among the five splits\n",
    "\n",
    "### We still see very good performance, and the same features are important throughout\n",
    "### How far can we go? Let's cross-validate on every sampled time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = metricsData['times'] # This is an attribute of the measured data set: the number of time points measured\n",
    "#churnModelMoreFolds = toolkit.refinement.makeAndUpdateModel(rootDirectory, metricsData['data'], folds, 'netchurn', modelInstance, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of this step are omitted for printing. However, the large cross-validation can be run to see them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The individual data sets used for training and testing are quite small and imbalanced.\n",
    "\n",
    "### Many of the same relationships still show up.\n",
    "\n",
    "### Why is 'added' a much more important factor than 'deleted'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print metricsData['data']['netchurn'].mean()\n",
    "print metricsData['data']['netchurn'].var()\n",
    "print metricsData['data']['netchurn'].std()\n",
    "print metricsData['data']['netchurn'].max()\n",
    "print metricsData['data']['netchurn'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This codebase is *growing* in general (more added than deleted)\n",
    "\n",
    "### Some files must experience more churn than others. We know from some of the motivating literature that defects can be correlated with large pre-release churn.\n",
    "\n",
    "### Let's make some categories of binned churn data and classify them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnBinnedCategories = ['churnLow','churnMedium','churnHigh','churnHigher', 'churnHighest']\n",
    "dataSetUpdated = toolkit.utilities.addBinnedResponseCategory(metricsData['data'], 'netchurn', churnBinnedCategories)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "modelInstance = DecisionTreeClassifier(max_leaf_nodes=8, criterion='entropy')\n",
    "churnModelCategories = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, churnBinnedCategories, modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're seeing something interesting. The vast majority of the files exhibit very low amounts of churn. A select few files receive most of the lines added/deleted. Does the class imbalance impact the validity of this model? Let's try more cross-validation to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "churnModelCategories = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, folds, churnBinnedCategories, modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at this from another point of view. What characterises the files which have the most lines added?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addedModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, metricsData['data'], 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net churn and deleted lines are strongly related. What do we find if we're not allowed to use these in our decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteredData = metricsData['data'].drop(['netchurn','deleted'], axis=1)\n",
    "addedModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model uses n-revs as the most important feature, but it does not classify '# lines added above the mean' very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteredData2 = metricsData['data'].drop(['netchurn','deleted','n-revs'], axis=1)\n",
    "addedModel2 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData2, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-authors has similar problems with identifying the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteredData3 = metricsData['data'].drop(['netchurn','deleted','n-revs','n-authors'], axis=1)\n",
    "addedModel3 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData3, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fractal-value is derived from n-revs and n-authors\n",
    "\n",
    "### Let's get rid of it and build a regression model for nline\n",
    "### This model will predict the sizes of files based on their other static features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "modelInstanceR = DecisionTreeRegressor(max_leaf_nodes=8)\n",
    "modelInstanceRsimpler = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "alteredData4 = metricsData['data'].drop(['netchurn','deleted','n-revs','n-authors','fractal-value'], axis=1)\n",
    "nlineModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData4, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the number of indented lines is a very good predictor of the number of lines\n",
    "\n",
    "### Is the model using indent_lines because it is correlated with nline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Spearman measure of rank correlation\n",
    "metricsData['data']['nline'].corr(metricsData['data']['indent_lines'], method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's remove indent_lines from this data set\n",
    "### Are we still able to regress on nline (and with high performance)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteredData5 = metricsData['data'].drop(['indent_lines','nchar','nstatement','nidentifier'], axis=1)\n",
    "nlineModelR2 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData5, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cqmetrics provides several measures of the 'number of functions' contained in a file (each calculated differently)\n",
    "\n",
    "### The model uses these to predict the size of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteredData6 = metricsData['data'].drop(['indent_lines','nchar','nstatement','nidentifier', 'nfunction','nfunction2','nfunction3'], axis=1)\n",
    "nlineModelR3 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData6, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alteredData7 = metricsData['data'].drop(['indent_lines','nchar','nstatement','nidentifier', 'nfunction','nfunction2','nfunction3', 'unique_nidentifier'], axis=1)\n",
    "nlineModelR4 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData7, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our regression model of nline needs the number of (unique) identifiers, and the number of functions in a file to explain the variance in our dataset\n",
    "\n",
    "### Without these features, the model rapidly loses accuracy.\n",
    "\n",
    "### From all the features we measure, the only ones which are strong predictors of size are other measures of such (which are bound to be correlated - lines of source code necessarily add identifiers, operators, functions etc as counted by Halstead's metrics)\n",
    "\n",
    "### We see that the maximum Halstead complexity metric among the functions in each file is a (weak) predictor under this model. The measures from which it is calculated have a much stronger correlation with size.\n",
    "\n",
    "### What about modelling these measures of complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycloData = metricsData['data'].drop(['cyclomatic_sd', 'cyclomatic_mean'], axis=1)\n",
    "cycloModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, cycloData, 2, 'cyclomatic_max', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclomatic complexity seems to be similar to Halstead complexity for our dataset\n",
    "### The measures which are used to derive both of these are also predictors. Let's remove them and repeat.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycloData2 = metricsData['data'].drop(['halstead_sd','nidentifier','halstead_mean','halstead_min','cyclomatic_sd', 'cyclomatic_mean', 'halstead_max','nstatement','statement_nesting_mean'], axis=1)\n",
    "cycloModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, cycloData2, 2, 'cyclomatic_max', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of static variables and the mean indentation level of files are strong predictors of cyclomatic complexity for our dataset\n",
    "\n",
    "### This indentation predictor is similar to the findings of Hindle. \n",
    "### What about the 'ninternal' (static linkage) result? The files containing functions with higher cyclomatic complexity also have more variables (which are shared between functions in the same file)?\n",
    "### This may be starting to give some insight into our codebase. Perhaps our code overuses file-global variables together with functions which are difficult to test.\n",
    "\n",
    "### Can we model indentation? What leads to 'wider' files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indentData = metricsData['data'].drop(['indent_sd','indent_median','indent_max','indent_lines'],axis=1)\n",
    "indentModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, indentData, 2, 'indent_mean', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The files with more nesting (which drives Halstead's complexity) are more indented\n",
    "\n",
    "### This tells us that our codebase uses indentation to *indicate* nesting frequently. This is typical in C programming, of course. However, there is value in this seemingly simple result: to locate the files with high syntax-driven measures of complexity in this codebase, we can use a heuristic like the level of indentation instead.\n",
    "\n",
    "### It is important to also consider that our dataset does not have other measures of complexity which have *not* been represented by this model.  Halstead and McCabe's measures are dominant in the measurement of C programs, but other measures of complexity which are not strongly connected with structural *nesting* may not be predicted by indentation. In other words, indentation does not necessarily predict *complexity* - it predicts *Halstead and McCabe complexity*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indentModelC = toolkit.refinement.makeAndUpdateModel(rootDirectory, indentData, 2, 'indent_mean', modelInstance, modelSimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use the DecisionTreeClassifier to bin samples above and below the mean indentation level, more effectively than we can predict the indentation level itself via regression.\n",
    "\n",
    "### To what degree is this true? Let's try adding more categories as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indentBinnedCategories = ['iLow','iMedium','iHigh','iVeryHigh']\n",
    "dataSetUpdated = toolkit.utilities.addBinnedResponseCategory(indentData, 'indent_mean', indentBinnedCategories)\n",
    "indentModelC = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, indentBinnedCategories, modelInstance, modelSimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model suffers from class imbalance: with only 8 leaves, it loses accuracy when predicting the files with the most indentation. This is a common issue with decision trees being exposed by our dataset. It is not a property of our codebase or of C source code.\n",
    "\n",
    "### In this case, we can use an *ensemble model* to make up for the loss of accuracy in the minority class. However, we do so at the expense of model interpretation. \n",
    "\n",
    "### We build a random forest: a group of decision trees are made with the first decision chosen at random. The group of trees is used to classify each sample, and a majority voting scheme decides the model output.\n",
    "\n",
    "### This has the effect of creating more trees. Some may resemble the above single tree (which accurately modelled *most* of our data). Others may be highly inaccurate, except for small subsets of the data. \n",
    "\n",
    "### The training of this model is 'embarassingly parallel': we use all the available CPU cores in parallel to create our decision trees.\n",
    "\n",
    "### Forest models can be difficult to tune, and to interpret. We omit visualization of the many trees. For automated classification tasks used in a production environment (as opposed to empirical research), forests may be valuable despite their lack of interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parallel construction of forest models\n",
    "import psutil\n",
    "cores = psutil.cpu_count()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# We build a forest of n_estimators trees, with no restriction on the breadth/depth of trees.\n",
    "modelCF = RandomForestClassifier(n_estimators=500, criterion='entropy', n_jobs=cores)\n",
    "modelCFsimpler = RandomForestClassifier(n_estimators=20, max_leaf_nodes=2, criterion='entropy', n_jobs=cores) # Each tree makes one decision\n",
    "updatedModelCF = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, indentBinnedCategories, modelCF, modelCFsimpler, scoreOnly=False, visualize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The resulting forest together uses many features. Many of these are named entities - our trees are individually modelling the files in our codebase. This showcases a possible threat to the use of forest models with our dataset: there may be a tendency to model the one-hot encoded entities.\n",
    "\n",
    "### We also note that the minority class is predicted with high Precision, but low Recall. From the high-indentation files we predict, they are predicted correctly. However, most of the high-indentation files are missed even by this model. The F1-Score (the harmonic mean of Precision and Recall) is similarly low.\n",
    "\n",
    "### Since the training of a random forest includes random choices (the initial splits in each tree), the results of this cell may vary each time it is run. The models before and after the update are likely to differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this case study, we have used the toolkit to do the following:\n",
    "- Gather the Git dataset\n",
    "- Create classification models of net churn: above and below the mean, in five binned categories\n",
    "- Create regression models of file size and cyclomatic complexity\n",
    "- Create regression and classification models of mean indentation \n",
    "    - Including a brief test of random forests to improve classification performance with a very small minority class we wish to predict: the files with the highest mean indentation level"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
