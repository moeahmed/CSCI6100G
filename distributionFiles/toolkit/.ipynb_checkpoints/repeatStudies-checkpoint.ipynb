{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseStudyAnalysis(dataSet):\n",
    "    #print dataSet['times']\n",
    "    #print dataSet['data'].shape\n",
    "    #print dataSet['data']['entity'].nunique()\n",
    "    #print dataSet['data'].columns\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    modelInstance = DecisionTreeClassifier(max_leaf_nodes=8, criterion='entropy')\n",
    "    modelSimpler = DecisionTreeClassifier(max_leaf_nodes=4, criterion='entropy')\n",
    "    print \"-- Churn model - 2 fold decision tree --\"\n",
    "    churnModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSet['data'], 2, 'netchurn', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    folds = 5\n",
    "    print \"-- Churn model - 5 fold decision tree --\"\n",
    "    churnModelMoreFolds = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSet['data'], folds, 'netchurn', modelInstance, modelSimpler, scoreOnly=False) \n",
    "\n",
    "    #print dataSet['data']['netchurn'].mean()\n",
    "    #print dataSet['data']['netchurn'].var()\n",
    "    #print dataSet['data']['netchurn'].std()\n",
    "    #print dataSet['data']['netchurn'].max()\n",
    "    #print dataSet['data']['netchurn'].min()\n",
    "\n",
    "    churnBinnedCategories = ['churnLow','churnMedium','churnHigh','churnHigher', 'churnHighest']\n",
    "    dataSetUpdated = toolkit.utilities.addBinnedResponseCategory(dataSet['data'], 'netchurn', churnBinnedCategories)\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    modelInstance = DecisionTreeClassifier(max_leaf_nodes=8, criterion='entropy')\n",
    "    print \"-- Churn model - 2 fold churnBinnedCategories--\"\n",
    "    churnModelCategories = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, churnBinnedCategories, modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    folds = 3\n",
    "    print \"-- Churn model - 3 fold churnBinnedCategories--\"\n",
    "    churnModelCategories = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, folds, churnBinnedCategories, modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    addedModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSet['data'], 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    alteredData = dataSet['data'].drop(['netchurn','deleted'], axis=1)\n",
    "    addedModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    alteredData2 = dataSet['data'].drop(['netchurn','deleted','n-revs'], axis=1)\n",
    "    addedModel2 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData2, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    alteredData3 = dataSet['data'].drop(['netchurn','deleted','n-revs','n-authors'], axis=1)\n",
    "    addedModel3 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData3, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    modelInstanceR = DecisionTreeRegressor(max_leaf_nodes=8)\n",
    "    modelInstanceRsimpler = DecisionTreeRegressor(max_leaf_nodes=4)    \n",
    "    alteredData4 = dataSet['data'].drop(['netchurn','deleted','n-revs','n-authors','fractal-value'], axis=1)\n",
    "    nlineModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData4, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    dataSet['data']['nline'].corr(dataSet['data']['indent_lines'], method='spearman')\n",
    "\n",
    "    alteredData5 = dataSet['data'].drop(['indent_lines','nchar','nstatement','nidentifier'], axis=1)\n",
    "    nlineModelR2 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData5, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    alteredData6 = dataSet['data'].drop(['indent_lines','nchar','nstatement','nidentifier', 'nfunction','nfunction2','nfunction3'], axis=1)\n",
    "    nlineModelR3 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData6, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    alteredData7 = dataSet['data'].drop(['indent_lines','nchar','nstatement','nidentifier', 'nfunction','nfunction2','nfunction3', 'unique_nidentifier'], axis=1)\n",
    "    nlineModelR4 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData7, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    cycloData = dataSet['data'].drop(['cyclomatic_sd', 'cyclomatic_mean'], axis=1)\n",
    "    cycloModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, cycloData, 2, 'cyclomatic_max', modelInstanceR, visualize=False, scoreOnly=False)\n",
    "\n",
    "    cycloData2 = dataSet['data'].drop(['halstead_sd','nidentifier','halstead_mean','halstead_min','cyclomatic_sd', 'cyclomatic_mean', 'halstead_max','nstatement','statement_nesting_mean'], axis=1)\n",
    "    cycloModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, cycloData2, 2, 'cyclomatic_max', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    indentData = dataSet['data'].drop(['indent_sd','indent_median','indent_max','indent_lines'],axis=1)\n",
    "    indentModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, indentData, 2, 'indent_mean', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    indentModelC = toolkit.refinement.makeAndUpdateModel(rootDirectory, indentData, 2, 'indent_mean', modelInstance, modelSimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    indentBinnedCategories = ['iLow','iMedium','iHigh','iVeryHigh']\n",
    "    dataSetUpdated = toolkit.utilities.addBinnedResponseCategory(indentData, 'indent_mean', indentBinnedCategories)\n",
    "    indentModelC = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, indentBinnedCategories, modelInstance, modelSimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    import psutil\n",
    "    cores = psutil.cpu_count()\n",
    "\n",
    "    print \"Random forest 2-fold\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    modelCF = RandomForestClassifier(n_estimators=500, criterion='entropy', n_jobs=cores)\n",
    "    modelCFsimpler = RandomForestClassifier(n_estimators=50, max_leaf_nodes=4, criterion='entropy', n_jobs=cores)    \n",
    "    updatedModelCF = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, indentBinnedCategories, modelCF, modelCFsimpler, scoreOnly=False, visualize=False)\n",
    "    \n",
    "    print \"Random forest 5-fold\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    modelCF = RandomForestClassifier(n_estimators=500, criterion='entropy', n_jobs=cores)\n",
    "    modelCFsimpler = RandomForestClassifier(n_estimators=50, max_leaf_nodes=4, criterion='entropy', n_jobs=cores)    \n",
    "    updatedModelCF = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 5, indentBinnedCategories, modelCF, modelCFsimpler, scoreOnly=False, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModels(x,y,folds):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    \n",
    "    import psutil\n",
    "    cores = psutil.cpu_count()\n",
    "    #folds = 5\n",
    "    estimators = 100\n",
    "\n",
    "    #print \"SVC\"\n",
    "    #clf = SVC(kernel='rbf')\n",
    "    #scoresCLF = cross_val_score(clf, x, y, cv=folds)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCLF.mean(), scoresCLF.std() * 2))\n",
    "\n",
    "    #print \"Naive Bayes\"\n",
    "    #gnb = GaussianNB()\n",
    "    #scoresGNB = cross_val_score(gnb, x, y, cv=folds)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresGNB.mean(), scoresGNB.std() * 2))\n",
    "\n",
    "    print \"Random Forest\"\n",
    "    modelRF = RandomForestClassifier(n_estimators=estimators, criterion='entropy', n_jobs=cores)\n",
    "    scoresRF = cross_val_score(modelRF, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresRF.mean(), scoresRF.std() * 2))\n",
    "\n",
    "    print \"Decision Tree\"\n",
    "    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scoresDT = cross_val_score(dt, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresDT.mean(), scoresDT.std() * 2))\n",
    "    \n",
    "    print \"Extremely Random Tree\"\n",
    "    edt = ExtraTreesClassifier(n_estimators=estimators, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scoresEDT = cross_val_score(edt, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresEDT.mean(), scoresEDT.std() * 2))\n",
    "    \n",
    "    #print \"MLP\"\n",
    "    #mlp = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(61, 30, 20, 15, 5, 1), random_state=1)\n",
    "    #scoresMLP = cross_val_score(mlp, x, y, cv=folds)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresMLP.mean(), scoresMLP.std() * 2))\n",
    "\n",
    "    #print \"AdaBoost\"\n",
    "    #ada = AdaBoostClassifier(n_estimators=estimators)\n",
    "    #scoresADA = cross_val_score(ada, x, y, cv=folds)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresADA.mean(), scoresADA.std() * 2))\n",
    "\n",
    "    #print \"SGD\"\n",
    "    #sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=20)\n",
    "    #scoresSGD = cross_val_score(sgd, x, y, cv=folds)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresSGD.mean(), scoresSGD.std() * 2)) \n",
    "    \n",
    "    #print \"Gradient Boost\"\n",
    "    #gb = GradientBoostingClassifier(n_estimators=estimators, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    #scoresGB = cross_val_score(gb, x, y, cv=folds)\n",
    "    #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresGB.mean(), scoresGB.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/vimStudy/'\n",
    "metricsDataVim = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/vim/vim', rootDirectory+'vim/', '*/*.c */*.h *.c *.h', ['indent','c'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataVim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely Random Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_val_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-30eeb22d8b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0medt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mscoresEDT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %0.2f (+/- %0.2f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscoresEDT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoresEDT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cross_val_score' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#x = metricsDataVim['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "x = metricsDataVim['data'].drop('netchurn', axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "#x = metricsDataVim['data'].drop(['entity','time','indent_lines','nchar','nstatement','nidentifier'], axis=1)\n",
    "y = metricsDataVim['data']['netchurn']\n",
    "\n",
    "#runModels(x,y,10)\n",
    "print \"Extremely Random Tree\"\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "edt = ExtraTreesClassifier(n_estimators=300, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scoresEDT = cross_val_score(edt, x, y, cv=folds)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresEDT.mean(), scoresEDT.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/openSSLstudy/'\n",
    "metricsDataOpenSSL = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/openssl/openssl', rootDirectory+'openssl/', '*/*.c */*.h *.c *.h', ['indent','c'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataOpenSSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = metricsDataOpenSSL['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataOpenSSL['data']['added'])\n",
    "runModels(x,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/httpdStudy/'\n",
    "metricsDataHTTPD = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/apache/httpd', rootDirectory+'httpd/', '*/*.c */*.h *.c *.h', ['indent','c'], branch='trunk', skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataHTTPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = metricsDataHTTPD['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataHTTPD['data']['added'])\n",
    "runModels(x,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/nginxStudy/'\n",
    "metricsDataNginx = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/nginx/nginx', rootDirectory+'nginx/', '*/*.c */*.h *.c *.h', ['indent','c'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataNginx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = metricsDataNginx['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataNginx['data']['added'])\n",
    "runModels(x,y,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
