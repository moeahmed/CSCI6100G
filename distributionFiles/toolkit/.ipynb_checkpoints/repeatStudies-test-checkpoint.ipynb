{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caseStudyAnalysis(dataSet):\n",
    "    #print dataSet['times']\n",
    "    #print dataSet['data'].shape\n",
    "    #print dataSet['data']['entity'].nunique()\n",
    "    #print dataSet['data'].columns\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    modelInstance = DecisionTreeClassifier(max_leaf_nodes=8, criterion='entropy')\n",
    "    modelSimpler = DecisionTreeClassifier(max_leaf_nodes=4, criterion='entropy')\n",
    "    print \"-- Churn model - 2 fold decision tree --\"\n",
    "    churnModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSet['data'], 2, 'netchurn', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    folds = 5\n",
    "    print \"-- Churn model - 5 fold decision tree --\"\n",
    "    churnModelMoreFolds = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSet['data'], folds, 'netchurn', modelInstance, modelSimpler, scoreOnly=False) \n",
    "\n",
    "    #print dataSet['data']['netchurn'].mean()\n",
    "    #print dataSet['data']['netchurn'].var()\n",
    "    #print dataSet['data']['netchurn'].std()\n",
    "    #print dataSet['data']['netchurn'].max()\n",
    "    #print dataSet['data']['netchurn'].min()\n",
    "\n",
    "    churnBinnedCategories = ['churnLow','churnMedium','churnHigh','churnHigher', 'churnHighest']\n",
    "    dataSetUpdated = toolkit.utilities.addBinnedResponseCategory(dataSet['data'], 'netchurn', churnBinnedCategories)\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    modelInstance = DecisionTreeClassifier(max_leaf_nodes=8, criterion='entropy')\n",
    "    print \"-- Churn model - 2 fold churnBinnedCategories--\"\n",
    "    churnModelCategories = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, churnBinnedCategories, modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    folds = 3\n",
    "    print \"-- Churn model - 3 fold churnBinnedCategories--\"\n",
    "    churnModelCategories = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, folds, churnBinnedCategories, modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    addedModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSet['data'], 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    alteredData = dataSet['data'].drop(['netchurn','deleted'], axis=1)\n",
    "    addedModel = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    alteredData2 = dataSet['data'].drop(['netchurn','deleted','n-revs'], axis=1)\n",
    "    addedModel2 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData2, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    alteredData3 = dataSet['data'].drop(['netchurn','deleted','n-revs','n-authors'], axis=1)\n",
    "    addedModel3 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData3, 2, 'added', modelInstance, modelSimpler, visualize=True, scoreOnly=False) \n",
    "\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    modelInstanceR = DecisionTreeRegressor(max_leaf_nodes=8)\n",
    "    modelInstanceRsimpler = DecisionTreeRegressor(max_leaf_nodes=4)    \n",
    "    alteredData4 = dataSet['data'].drop(['netchurn','deleted','n-revs','n-authors','fractal-value'], axis=1)\n",
    "    nlineModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData4, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    dataSet['data']['nline'].corr(dataSet['data']['indent_lines'], method='spearman')\n",
    "\n",
    "    alteredData5 = dataSet['data'].drop(['indent_lines','nchar','nstatement','nidentifier'], axis=1)\n",
    "    nlineModelR2 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData5, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    alteredData6 = dataSet['data'].drop(['indent_lines','nchar','nstatement','nidentifier', 'nfunction','nfunction2','nfunction3'], axis=1)\n",
    "    nlineModelR3 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData6, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    alteredData7 = dataSet['data'].drop(['indent_lines','nchar','nstatement','nidentifier', 'nfunction','nfunction2','nfunction3', 'unique_nidentifier'], axis=1)\n",
    "    nlineModelR4 = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData7, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    cycloData = dataSet['data'].drop(['cyclomatic_sd', 'cyclomatic_mean'], axis=1)\n",
    "    cycloModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, cycloData, 2, 'cyclomatic_max', modelInstanceR, visualize=False, scoreOnly=False)\n",
    "\n",
    "    cycloData2 = dataSet['data'].drop(['halstead_sd','nidentifier','halstead_mean','halstead_min','cyclomatic_sd', 'cyclomatic_mean', 'halstead_max','nstatement','statement_nesting_mean'], axis=1)\n",
    "    cycloModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, cycloData2, 2, 'cyclomatic_max', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    indentData = dataSet['data'].drop(['indent_sd','indent_median','indent_max','indent_lines'],axis=1)\n",
    "    indentModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, indentData, 2, 'indent_mean', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    indentModelC = toolkit.refinement.makeAndUpdateModel(rootDirectory, indentData, 2, 'indent_mean', modelInstance, modelSimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    indentBinnedCategories = ['iLow','iMedium','iHigh','iVeryHigh']\n",
    "    dataSetUpdated = toolkit.utilities.addBinnedResponseCategory(indentData, 'indent_mean', indentBinnedCategories)\n",
    "    indentModelC = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, indentBinnedCategories, modelInstance, modelSimpler, visualize=False, scoreOnly=False)\n",
    "\n",
    "    import psutil\n",
    "    cores = psutil.cpu_count()\n",
    "\n",
    "    print \"Random forest 2-fold\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    modelCF = RandomForestClassifier(n_estimators=500, criterion='entropy', n_jobs=cores)\n",
    "    modelCFsimpler = RandomForestClassifier(n_estimators=50, max_leaf_nodes=4, criterion='entropy', n_jobs=cores)    \n",
    "    updatedModelCF = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 2, indentBinnedCategories, modelCF, modelCFsimpler, scoreOnly=False, visualize=False)\n",
    "    \n",
    "    print \"Random forest 5-fold\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    modelCF = RandomForestClassifier(n_estimators=500, criterion='entropy', n_jobs=cores)\n",
    "    modelCFsimpler = RandomForestClassifier(n_estimators=50, max_leaf_nodes=4, criterion='entropy', n_jobs=cores)    \n",
    "    updatedModelCF = toolkit.refinement.makeAndUpdateModel(rootDirectory, dataSetUpdated, 5, indentBinnedCategories, modelCF, modelCFsimpler, scoreOnly=False, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModels(x,y,folds):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    \n",
    "    import psutil\n",
    "    cores = psutil.cpu_count()\n",
    "    #folds = 5\n",
    "    estimators = 100\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print \"SVC\"\n",
    "    clf = SVC(kernel='rbf')\n",
    "    scoresCLF = cross_val_score(clf, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresCLF.mean(), scoresCLF.std() * 2))\n",
    "'''\n",
    "    print \"Naive Bayes\"\n",
    "    gnb = GaussianNB()\n",
    "    scoresGNB = cross_val_score(gnb, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresGNB.mean(), scoresGNB.std() * 2))\n",
    "\n",
    "    print \"Random Forest\"\n",
    "    modelRF = RandomForestClassifier(n_estimators=estimators, criterion='entropy', n_jobs=cores)\n",
    "    scoresRF = cross_val_score(modelRF, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresRF.mean(), scoresRF.std() * 2))\n",
    "\n",
    "    print \"Decision Tree\"\n",
    "    dt = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scoresDT = cross_val_score(dt, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresDT.mean(), scoresDT.std() * 2))\n",
    "    \n",
    "    print \"Extremely Random Tree\"\n",
    "    edt = ExtraTreesClassifier(n_estimators=estimators, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    scoresEDT = cross_val_score(edt, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresEDT.mean(), scoresEDT.std() * 2))\n",
    "    '''\n",
    "    print \"MLP\"\n",
    "    mlp = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(61, 30, 20, 15, 5, 1), random_state=1)\n",
    "    scoresMLP = cross_val_score(mlp, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresMLP.mean(), scoresMLP.std() * 2))\n",
    "\n",
    "    print \"AdaBoost\"\n",
    "    ada = AdaBoostClassifier(n_estimators=estimators)\n",
    "    scoresADA = cross_val_score(ada, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresADA.mean(), scoresADA.std() * 2))\n",
    "\n",
    "    print \"SGD\"\n",
    "    sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=20)\n",
    "    scoresSGD = cross_val_score(sgd, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresSGD.mean(), scoresSGD.std() * 2)) \n",
    "    \n",
    "    print \"Gradient Boost\"\n",
    "    gb = GradientBoostingClassifier(n_estimators=estimators, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    scoresGB = cross_val_score(gb, x, y, cv=folds)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresGB.mean(), scoresGB.std() * 2))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-60f862ef38af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrootDirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../dataSets/vimStudy/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetricsDataVim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgatherTimeMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootDirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'https://github.com/vim/vim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrootDirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'vim/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*/*.c */*.h *.c *.h'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipEvery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#caseStudyAnalysis(metricsDataVim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/work/toolkit/dataGathering.pyc\u001b[0m in \u001b[0;36mgatherTimeMetrics\u001b[0;34m(studyDirectory, repositoryURL, sourceFilesDirectory, filesToInspect, language, branch, skipEvery, replaceMissing)\u001b[0m\n\u001b[1;32m     54\u001b[0m                         \u001b[0mtimeDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtimeMetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeDataSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtimeMetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreplaceMissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/pandas/core/reshape/concat.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                        copy=copy, sort=sort)\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python2/lib/python2.7/site-packages/pandas/core/reshape/concat.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "rootDirectory = '../dataSets/vimStudy/'\n",
    "metricsDataVim = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/vim/vim', rootDirectory+'vim/', '*/*.c */*.h *.c *.h', ['indent','c'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataVim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = metricsDataVim['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataVim['data']['added'])\n",
    "runModels(x,y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#x = metricsDataVim['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "x = metricsDataVim['data'].drop('netchurn', axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "#x = metricsDataVim['data'].drop(['entity','time','indent_lines','nchar','nstatement','nidentifier'], axis=1)\n",
    "y = metricsDataVim['data']['netchurn']\n",
    "\n",
    "#runModels(x,y,10)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "x2 = metricsDataVim['data'].drop(['netchurn','deleted','n-revs','n-authors','fractal-value'], axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "\n",
    "print \"MLP\"\n",
    "mlp = MLPClassifier(solver='adam', alpha=1e-8, hidden_layer_sizes=(512,256,128), random_state=1)\n",
    "scoresMLP = cross_val_score(mlp, x2, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scoresMLP.mean(), scoresMLP.std() * 2))\n",
    "    \n",
    "#nlineModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData4, 2, 'nline', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/openSSLstudy/'\n",
    "metricsDataOpenSSL = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/openssl/openssl', rootDirectory+'openssl/', '*/*.c */*.h *.c *.h', ['indent','c'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataOpenSSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "modelInstanceR = DecisionTreeRegressor(max_leaf_nodes=8)\n",
    "modelInstanceRsimpler = DecisionTreeRegressor(max_leaf_nodes=4)\n",
    "alteredData4 = metricsDataOpenSSL['data']\n",
    "nlineModelR = toolkit.refinement.makeAndUpdateModel(rootDirectory, alteredData4, 3, 'cbo', modelInstanceR, modelInstanceRsimpler, visualize=False, scoreOnly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = metricsDataOpenSSL['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataOpenSSL['data']['added'])\n",
    "runModels(x,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/httpdStudy/'\n",
    "metricsDataHTTPD = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/apache/httpd', rootDirectory+'httpd/', '*/*.c */*.h *.c *.h', ['indent','c'], branch='trunk', skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataHTTPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = metricsDataHTTPD['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataHTTPD['data']['added'])\n",
    "runModels(x,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = '../dataSets/nginxStudy/'\n",
    "metricsDataNginx = toolkit.data.gatherTimeMetrics(rootDirectory, 'https://github.com/nginx/nginx', rootDirectory+'nginx/', '*/*.c */*.h *.c *.h', ['indent','c'], skipEvery=50)\n",
    "#caseStudyAnalysis(metricsDataNginx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = metricsDataNginx['data'].drop('netchurn',axis=1).drop('time', axis=1).drop('entity',axis=1)\n",
    "y = np.array(metricsDataNginx['data']['added'])\n",
    "runModels(x,y,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
