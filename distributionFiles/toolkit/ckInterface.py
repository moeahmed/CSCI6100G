import os
#import os.path
import subprocess
import pandas as pd
import csv
#import numpy as np
import pdb

#Path to .jar to run ck (Tool that gathers java metrics)

def collectMetrics(ckDir, repoPath):
    #Generates metrics on all java files located in repoPath or in subdirectories under repoPath
    #This method runs the .jar that generates the raw .csv files

    #First, generate the .csvs by running the jar
    jarPath = os.path.join(ckDir,"ck-0.4.5-SNAPSHOT-jar-with-dependencies.jar")
    runJar(jarPath,repoPath)

    #The CSVs will be generated in the current working directory
    readPath = jarPath = os.path.join(os.getcwd(), "class.csv")
    writePath = os.path.join(os.getcwd(),"tmp.csv")
    
    #Process the raw CSV, so that metrics are listed by file, not by class. Save this information into tmp.csv
    processRawCSV(readPath,writePath)

    #TODO- How to import this process CSV into a pandas dataframe?
    metricsData = pd.read_csv(writePath, names=featureHeader)
    
    return metricsData.rename(columns={"file":"entity"})
    
    
def dropUnusedMetrics(metricsTable):
	for name in metricsTable.columns:
		if name not in usedFeatureNames and name !="file":
			metricsTable.drop(name, axis=1, inplace=True)

	return metricsTable

def runJar(jarPath, repoPath):
    command = "java -jar "+jarPath+" "+repoPath+" false"
    try:
        output = os.system(command)
        print(output)
    except:
        print("Error: Could not run jar file.")
        return

#collectMetrics(jarPath,repoPath)    
def refineRow(row):

    for i in range(3, len(featureHeader)):
        row[i]=int(row[i])
    
    rep = os.path.join(os.path.basename(os.path.dirname(row[0])), os.path.basename(row[0]))
    #rep = os.path.basename(row[0])
    row[0]=rep
    
    print("filename",rep)     
     
   
    return row

def countCSVRows(filename):
    command = "cat "+filename+"  | wc -l"
    output = subprocess.check_output(command, shell=True)
    return int(output)

def processRawCSV(readPath, writePath):
    # This method takes the .csv generated by ck and transforms it into a csv where each row represents a unique java file
    # This is so the CodeMaat data and CK data are presented file by file and there is a 1 to 1 mapping

    with open(readPath, 'rb') as readFrom:
        with open(writePath,'wb') as writeFrom:
            reader = csv.reader(readFrom, delimiter=',', quotechar='|')
            writer = csv.writer(writeFrom, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
            
            #Get the number of rows in the CSV
            
            
            nRows = countCSVRows(readPath)
            
            #pdb.set_trace()
            
            #Copy the headers
            row = reader.next()
            writer.writerow(row)

            rowLength = len(featureHeader)
            rows = 1

            isNext = True
           


            #Go through each row of the csv and combine rows that have the same file name
            while(rows < nRows):
                #If we need to pull the next row
                if(isNext):
                    currRow = refineRow(reader.next())
                    rows+=1
                else:
                    currRow = nextRow

                isNext = True
                    
                #If there is another row to compare to
                if(rows < nRows-1):    
                    nextRow = refineRow(reader.next())
                    rows+=1
                    s1=str(currRow[0]) 
                    s2 = str(nextRow[0])

                    b= s1 == s2

                    
                    #Let's compare the next row     
                    while(currRow[0]==nextRow[0] and rows<nRows):
                        #Combine all the rows one can combine
                        for i in range(3, rowLength):
                            
                            currRow[i]+=nextRow[i]

                        #Move to the next row
                        nextRow = refineRow(next(reader))
                        rows+=1

                    #We need to figure out if the nextRow should replace the currRow    
                    if(currRow[0] != nextRow[0]):
                        isNext = False
                        
                #Write out the row to the csv file
                writer.writerow(currRow)
                

                
def codeMetricsTable(nameLabel, dataFrame, sourceFilesDirectory, storageConnection):
	import storage
	import scmData as scm
	import dataUtilities

	tableName = nameLabel + '_codeMetrics'
	if storage.tableExists(tableName, storageConnection):
		return storage.readTable(tableName, storageConnection)

	scm.gitlog.switchToRevision(sourceFilesDirectory,nameLabel)

	
	metricsData = collectMetrics(os.path.join(os.getcwd(),"..","ck"), sourceFilesDirectory)

	#if not metricsData:
		#return None

	#metricsData = pd.concat(metricsData)

	metricsData = dropUnusedMetrics(metricsData)
	#metricsDataFrame = metricsData.rename(columns={"file":"entity"})
	print("metricsData - before:",metricsData)

	#metricsData = dataUtilities.formatEntityNames(metricsData, sourceFilesDirectory)

	storage.writeTable(tableName, storageConnection, metricsData)

	print("metricsData - after:",metricsData)
	return metricsData




usedFeatureNames = ["entity","cbo","wmc",	"dit",	"rfc","lcom","totalMethods","staticMethods","publicMethods","privateMethods","protectedMethods",
                 "defaultMethods","abstractMethods", "finalMethods","synchronizedMethods","totalFields","staticFields","publicFields","privateFields","protectedFields",
                 "defaultFields","finalFields","synchronizedFields","nosi","loc","returnQty","loopQty","comparisonsQty","tryCatchQty","parenthesizedExpsQty","stringLiteralsQty",
                 "numbersQty","assignmentsQty","mathOperationsQty","variablesQty","maxNestedBlocks","anonymousClassesQty","subClassesQty","lambdasQty","uniqueWordsQty","modifiers"]

featureHeader = ["file","class"	,"type"	,"cbo",	"wmc",	"dit",	"rfc","lcom","totalMethods","staticMethods","publicMethods","privateMethods","protectedMethods",
                 "defaultMethods","abstractMethods", "finalMethods","synchronizedMethods","totalFields","staticFields","publicFields","privateFields","protectedFields",
                 "defaultFields","finalFields","synchronizedFields","nosi","loc",	"returnQty","loopQty","comparisonsQty","tryCatchQty","parenthesizedExpsQty","stringLiteralsQty",
                 "numbersQty","assignmentsQty","mathOperationsQty","variablesQty","maxNestedBlocks","anonymousClassesQty","subClassesQty","lambdasQty","uniqueWordsQty","modifiers"]


#processRawCSV(csvPath, newCSVPath)
